# Spark Project with Docker

## Этот проект представляет собой приложение Apache Spark, которое выполняет обработку данных с использованием Python (PySpark). Проект упакован в Docker-образ для удобства развертывания и выполнения.

## Описание проекта
Проект использует Apache Spark для обработки данных. Он может быть запущен как локально, так и в Docker-контейнере. В текущей реализации приложение выполняет задачи, связанные с анализом данных (например, группировка или агрегация).

## Требования
Для работы с проектом вам понадобятся:
* `Docker` : Для сборки и запуска контейнера.
* `Python 3.9+` : Для локальной разработки.
* `Apache Spark 3.5.5` : Для локального выполнения Spark-задач.
* `Java Development Kit (JDK) 17` : Необходим для работы Spark.

# Установка

## Клонирование репозитория

Склонируйте репозиторий на ваш компьютер:

```
git clone https://github.com/qqwskha/test_task2 
cd test2
```

## Создание Docker-образа

Соберите Docker-образ с помощью команды:

```
docker build --progress=plain -t pyspark-app .
```

## Запуск

### 1. Запуск через Docker

Чтобы запустить приложение в Docker-контейнере, выполните:

```docker run pyspark-app```

### 2. Локальный запуск (без Docker)

Если вы хотите запустить приложение локально:

1. Убедитесь, что установлены Python и Spark.
2. Установите зависимости:
```
pip install -r requirements.txt
```
3. Запустите приложение:

```
spark-submit main.py
```

## Логирование

Приложение выводит логи Spark, которые содержат информационные сообщения (`INFO`) и предупреждения (`WARN`). Чтобы очистить логи от служебных сообщений, используйте следующие команды:
### Для Unix/Linux/macOS:
```bash
docker logs <container_id> | grep -v "INFO"
```

### Для Windows PowerShell:

```powershell
docker logs <container_id> | Select-String -Pattern "INFO" -NotMatch
```

Вы также можете сохранить очищенные логи в файл:
```
docker logs <container_id> > output.txt
```



